<p align="center">
  <img src="pheme-hero.png" alt="Pheme â€” AI-powered daily news digest" width="700">
</p>

<h1 align="center">
  <img src="pheme-icon.png" alt="" width="32" height="32" style="vertical-align: middle;">
  Pheme
</h1>

<p align="center">
  <em>Named after the Greek goddess of fame and report â€” she gathers, summarizes, and delivers your daily news digest.</em>
</p>

<p align="center">
  <a href="#features">Features</a> Â· <a href="#quick-start">Quick Start</a> Â· <a href="#docker">Docker</a> Â· <a href="#admin-ui">Admin UI</a> Â· <a href="#api-reference">API</a> Â· <a href="#configuration">Configuration</a>
</p>

---

**Pheme** is a self-hosted Python service that aggregates news from RSS feeds, Reddit, and web pages, summarizes articles using a local LLM via [Ollama](https://ollama.com), and delivers a curated HTML digest by email every morning.

She runs entirely on your own hardware â€” no cloud APIs, no tracking, no subscriptions.

## Features

- **Multi-source aggregation** â€” RSS/Atom feeds, Reddit subreddits, and generic web scraping via a pluggable fetcher architecture
- **Local LLM summarization** â€” powered by [Ollama](https://ollama.com); runs any model you choose (default: `qwen2.5:1.5b-instruct`)
- **Topic-based digests** â€” organize sources into topics with keyword matching, regex patterns, and priority-based ranking
- **Cross-topic deduplication** â€” each article appears in only one section, assigned to the highest-scoring topic
- **Keyword filtering** â€” global blocklist to suppress articles matching unwanted keywords, with configurable scope (title+preview or full text)
- **Full-text extraction** â€” fetches complete article content for better summaries, not just RSS snippets
- **Scheduled delivery** â€” daily email via APScheduler cron (default: 06:00 UTC)
- **Admin UI** â€” built-in dark-themed web interface for managing sources, topics, keyword blocklist, and triggering digests
- **Comprehensive tests** â€” 230+ tests with 80%+ coverage target

## Architecture

```
RSS Feeds â”€â”
Reddit â”€â”€â”€â”€â”¤â”€â”€ [Fetchers]  â”€â”€ [Full-text Extract] â”€â”€ [Keyword Filter]
Web Pages â”€â”˜   Strategy +      BeautifulSoup          Global blocklist
               Factory                                 (configurable)

                    â†“

              [Topic Matching] â”€â”€ [Dedup] â”€â”€ [Summarizer] â†’ [Composer] â†’ ðŸ“¬
              Keyword + regex     One article   Ollama LLM    Jinja2 HTML
              scoring             per section   (local)       + plain text

                    â†“

              [Scheduler]      APScheduler cron (daily at 06:00)
```

## Quick Start

### Prerequisites

- **Python 3.12+**
- An [Ollama](https://ollama.com) instance with a model pulled:
  ```bash
  ollama pull qwen2.5:1.5b-instruct
  ```
- SMTP credentials for email delivery (Gmail [app passwords](https://support.google.com/accounts/answer/185833) work well)

### Install & Run

```bash
# Clone
git clone https://github.com/oveku/pheme.git
cd pheme

# Virtual environment
python -m venv .venv
source .venv/bin/activate      # Linux / macOS
.venv\Scripts\activate         # Windows

pip install -r requirements.txt

# Configure
cp .env.example .env
# Edit .env with your SMTP credentials and Ollama host

# Seed example sources (optional)
python seed.py

# Start
uvicorn app.main:app --host 0.0.0.0 --port 8020
```

Pheme is now running at **http://localhost:8020**. Visit the admin UI at **/admin**.

## Docker

The simplest way to run Pheme in production:

```bash
# Configure
cp .env.example .env
# Edit .env with your settings

# Start
docker compose up -d

# Seed example sources (optional)
docker compose exec pheme python seed.py
```

### Connecting to an External Ollama Host

If Ollama runs on another machine on your network:

```dotenv
OLLAMA_HOST=http://your-ollama-host:11434
```

## Admin UI

Pheme includes a built-in admin interface at `/admin` for managing your digest without touching the API:

| Page | Path | What You Can Do |
|------|------|-----------------|
| Dashboard | `/admin` | Overview of sources, topics, and recent digests |
| Sources | `/admin/sources` | Add, view, and delete RSS/Reddit/web sources |
| Topics | `/admin/topics` | Create topics with keywords, regex patterns, and priorities |
| Digest | `/admin/digest` | Trigger a manual digest run and view send history || Settings | `/admin/settings` | Manage blocked keywords and configure filter scope |
## API Reference

### Sources

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/sources` | List all sources |
| `POST` | `/api/sources` | Add a source |
| `GET` | `/api/sources/{id}` | Get source details |
| `PUT` | `/api/sources/{id}` | Update a source |
| `DELETE` | `/api/sources/{id}` | Remove a source |

### Topics

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/topics` | List all topics |
| `POST` | `/api/topics` | Create a topic |
| `GET` | `/api/topics/{id}` | Get topic details |
| `PUT` | `/api/topics/{id}` | Update a topic |
| `DELETE` | `/api/topics/{id}` | Remove a topic |
| `GET` | `/api/topics/{id}/sources` | List sources for a topic |

### Digest

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/api/digest/history` | Digest send history |
| `POST` | `/api/digest/trigger` | Manually trigger a digest |
| `GET` | `/health` | Health check |

### Examples

```bash
# Add an RSS source
curl -X POST http://localhost:8020/api/sources \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Hacker News",
    "type": "rss",
    "url": "https://hnrss.org/best",
    "category": "tech",
    "config": {"max_items": 15}
  }'

# Add a Reddit source
curl -X POST http://localhost:8020/api/sources \
  -H "Content-Type: application/json" \
  -d '{
    "name": "r/MachineLearning",
    "type": "reddit",
    "url": "r/MachineLearning",
    "category": "ai",
    "config": {"sort": "hot", "limit": 10}
  }'

# Create a topic with keyword matching
curl -X POST http://localhost:8020/api/topics \
  -H "Content-Type: application/json" \
  -d '{
    "name": "AI & Machine Learning",
    "keywords": ["AI", "machine learning", "LLM", "neural network", "GPT"],
    "priority": 80,
    "max_articles": 10
  }'

# Trigger a digest
curl -X POST http://localhost:8020/api/digest/trigger
```

## Configuration

All settings via environment variables or `.env` file:

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama API endpoint |
| `OLLAMA_MODEL` | `qwen2.5:1.5b-instruct` | LLM model for summarization |
| `SMTP_HOST` | `smtp.gmail.com` | SMTP server hostname |
| `SMTP_PORT` | `587` | SMTP port (STARTTLS) |
| `SMTP_USER` | â€” | Sender email address |
| `SMTP_PASSWORD` | â€” | SMTP password / app password |
| `DIGEST_RECIPIENT` | â€” | Recipient email address |
| `DIGEST_CRON_HOUR` | `6` | Digest send hour |
| `DIGEST_CRON_MINUTE` | `0` | Digest send minute |
| `DIGEST_TIMEZONE` | `UTC` | Scheduler timezone |
| `PHEME_PORT` | `8020` | HTTP server port |
| `PHEME_DB_PATH` | `./pheme.sqlite` | SQLite database path |

## Testing

```bash
# Run all tests
python -m pytest tests/ -v

# With coverage report
python -m pytest tests/ --cov=app --cov-report=term-missing

# By category
python -m pytest tests/ -m unit
python -m pytest tests/ -m api
python -m pytest tests/ -m fetcher
python -m pytest tests/ -m pipeline
```

## Design Patterns

Pheme uses several classic design patterns:

| Pattern | Where | Purpose |
|---------|-------|---------|
| **Strategy** | `fetchers/`, `matching.py` | Interchangeable fetchers; configurable filter scope |
| **Factory** | `FetcherFactory` | Creates the correct fetcher from source type |
| **Template Method** | `BaseFetcher.fetch()` | Defines connect â†’ extract â†’ normalize skeleton |
| **Singleton** | `config.py`, `database.py` | Settings and DB connection managed as singletons |
| **Pipeline** | `DigestPipeline` | Orchestrates fetch â†’ extract â†’ filter â†’ match â†’ dedup â†’ summarize â†’ email |

## Project Structure

```
pheme/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/           # REST API routes (sources, topics, digest)
â”‚   â”œâ”€â”€ email/         # HTML/plain-text composer + SMTP sender
â”‚   â”œâ”€â”€ fetchers/      # RSS, Reddit, Web fetchers + factory
â”‚   â”œâ”€â”€ pipeline/      # Digest orchestrator + topic matching + filtering + dedup
â”‚   â”œâ”€â”€ scheduler/     # APScheduler cron job definitions
â”‚   â”œâ”€â”€ static/        # Icons and static assets
â”‚   â”œâ”€â”€ summarizer/    # Ollama LLM client with fallback
â”‚   â”œâ”€â”€ templates/     # Jinja2 email templates
â”‚   â”œâ”€â”€ ui/            # Built-in admin web interface
â”‚   â”œâ”€â”€ config.py      # Pydantic settings from environment
â”‚   â”œâ”€â”€ database.py    # async SQLite CRUD layer
â”‚   â”œâ”€â”€ main.py        # FastAPI app with lifespan management
â”‚   â””â”€â”€ models.py      # Pydantic data models
â”œâ”€â”€ tests/             # 230+ tests (pytest + pytest-asyncio)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ seed.py            # Default source seeder
â””â”€â”€ .env.example       # Configuration template
```

## Etymology

> **Pheme** (Î¦Î®Î¼Î·) was the Greek goddess â€” and personification â€” of fame, rumour, and report. She was described as having many eyes and mouths, always watching and always speaking. Fitting for a service that watches dozens of news sources and reports back with a tidy summary each morning.

## License

[MIT](LICENSE)
